{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torcheval.metrics import MulticlassAccuracy, MulticlassF1Score, MulticlassRecall, MulticlassPrecision, MulticlassConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3, 6, 5\n",
    "        )  # (3, 256, 256) -> new dim (256 - 5 + (2 * 0)) / 1 + 1 ) = 252 (6, 252, 252)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # (6, 252, 252) -> (6, 126, 126)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            6, 16, 5\n",
    "        )  # (6, 126, 126) -> (16, 122, 122) -> MaxPool -> (16, 61, 61)\n",
    "        # self.fc1 = nn.Linear(16 * 61 * 61, 2048)\n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 128)\n",
    "        self.fc3 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataloaders(dataset):\n",
    "    # data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    data_loader = DataLoader(dataset, shuffle=True)\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    # print(images, labels)\n",
    "    print(dataset.classes)\n",
    "    # imshow(torchvision.utils.make_grid(images))\n",
    "    labels = np.array([dataset.targets[i] for i in range(len(dataset))])\n",
    "\n",
    "    # Define split sizes\n",
    "    train_size = 0.7  # 70% for training\n",
    "    val_size = 0.15  # 15% for validation\n",
    "    test_size = 0.15  # 15% for testing\n",
    "\n",
    "    # First, split into train + temp (val + test)\n",
    "    train_idx, temp_idx, _, temp_labels = train_test_split(\n",
    "        np.arange(len(dataset)),\n",
    "        labels,\n",
    "        stratify=labels,\n",
    "        test_size=(1 - train_size),\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # Then, split temp into validation and test\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx,\n",
    "        stratify=temp_labels,\n",
    "        test_size=(test_size / (test_size + val_size)),\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # Create samplers\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler)\n",
    "    # test_loader = DataLoader(dataset, batch_size=32, sampler=test_sampler)\n",
    "    test_loader = DataLoader(dataset, sampler=test_sampler)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def load_dataset(directory_path):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize((64,64)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    dataset = torchvision.datasets.ImageFolder(root=directory_path, transform=transform)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple_Black_rot', 'Apple_healthy', 'Apple_rust', 'Apple_scab']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('~/sgoinfre/leaves/images/Apple')\n",
    "train_loader, val_loader, test_loader = create_dataloaders(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_metrics(net, validation_loader):\n",
    "    net.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    validation_metrics = {'loss': 0}\n",
    "    with torch.no_grad():\n",
    "        f1_score = MulticlassF1Score(num_classes=4, average=\"macro\")\n",
    "        for inputs, labels in validation_loader:\n",
    "            outputs = net(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            validation_metrics[\"loss\"] += criterion(outputs, labels)\n",
    "            f1_score.update(preds, labels)\n",
    "    validation_metrics[\"loss\"] /= len(validation_loader)\n",
    "    validation_metrics[\"f1_score\"] = f1_score.compute()\n",
    "    net.train()\n",
    "    return validation_metrics\n",
    "\n",
    "def update_validation_metrics_history(net, validation_loader, validation_metrics_history):\n",
    "    new_validation_metrics = compute_validation_metrics(net, validation_loader)\n",
    "    for name, value in new_validation_metrics.items():\n",
    "        validation_metrics_history[name].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, validation_loader):\n",
    "    net = CNN()\n",
    "    print(net)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    optimizer = optim.AdamW(net.parameters(), lr=0.001)\n",
    "    validation_metrics_history = {\n",
    "        \"f1_score\": [],\n",
    "        \"loss\": [],\n",
    "        \"accuracy\": []\n",
    "    }\n",
    "    for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            print(i)\n",
    "        print(validation_metrics_history)\n",
    "        update_validation_metrics_history(net, validation_loader, validation_metrics_history)\n",
    "        print(validation_metrics_history)\n",
    "        print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=2704, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "{'f1_score': [], 'loss': [], 'accuracy': []}\n",
      "{'f1_score': [tensor(0.5727)], 'loss': [tensor(0.5967)], 'accuracy': []}\n",
      "[1,    35] loss: 0.017\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "{'f1_score': [tensor(0.5727)], 'loss': [tensor(0.5967)], 'accuracy': []}\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406)], 'loss': [tensor(0.5967), tensor(0.3684)], 'accuracy': []}\n",
      "[2,    35] loss: 0.009\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406)], 'loss': [tensor(0.5967), tensor(0.3684)], 'accuracy': []}\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475)], 'accuracy': []}\n",
      "[3,    35] loss: 0.006\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475)], 'accuracy': []}\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276)], 'accuracy': []}\n",
      "[4,    35] loss: 0.005\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276)], 'accuracy': []}\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190), tensor(0.9052)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276), tensor(0.2304)], 'accuracy': []}\n",
      "[5,    35] loss: 0.004\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190), tensor(0.9052)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276), tensor(0.2304)], 'accuracy': []}\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190), tensor(0.9052), tensor(0.9233)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276), tensor(0.2304), tensor(0.1851)], 'accuracy': []}\n",
      "[6,    35] loss: 0.003\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190), tensor(0.9052), tensor(0.9233)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276), tensor(0.2304), tensor(0.1851)], 'accuracy': []}\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190), tensor(0.9052), tensor(0.9233), tensor(0.9332)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276), tensor(0.2304), tensor(0.1851), tensor(0.1513)], 'accuracy': []}\n",
      "[7,    35] loss: 0.003\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190), tensor(0.9052), tensor(0.9233), tensor(0.9332)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276), tensor(0.2304), tensor(0.1851), tensor(0.1513)], 'accuracy': []}\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190), tensor(0.9052), tensor(0.9233), tensor(0.9332), tensor(0.9195)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276), tensor(0.2304), tensor(0.1851), tensor(0.1513), tensor(0.1744)], 'accuracy': []}\n",
      "[8,    35] loss: 0.003\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190), tensor(0.9052), tensor(0.9233), tensor(0.9332), tensor(0.9195)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276), tensor(0.2304), tensor(0.1851), tensor(0.1513), tensor(0.1744)], 'accuracy': []}\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190), tensor(0.9052), tensor(0.9233), tensor(0.9332), tensor(0.9195), tensor(0.9413)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276), tensor(0.2304), tensor(0.1851), tensor(0.1513), tensor(0.1744), tensor(0.1521)], 'accuracy': []}\n",
      "[9,    35] loss: 0.003\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190), tensor(0.9052), tensor(0.9233), tensor(0.9332), tensor(0.9195), tensor(0.9413)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276), tensor(0.2304), tensor(0.1851), tensor(0.1513), tensor(0.1744), tensor(0.1521)], 'accuracy': []}\n",
      "{'f1_score': [tensor(0.5727), tensor(0.8406), tensor(0.9009), tensor(0.9190), tensor(0.9052), tensor(0.9233), tensor(0.9332), tensor(0.9195), tensor(0.9413), tensor(0.9361)], 'loss': [tensor(0.5967), tensor(0.3684), tensor(0.2475), tensor(0.2276), tensor(0.2304), tensor(0.1851), tensor(0.1513), tensor(0.1744), tensor(0.1521), tensor(0.1645)], 'accuracy': []}\n",
      "[10,    35] loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "net = train_model(train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, test_loader, num_classes):\n",
    "    net.eval()\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": MulticlassAccuracy(),\n",
    "        \"recall\": MulticlassRecall(num_classes=num_classes, average=\"macro\"),\n",
    "        \"precision\": MulticlassPrecision(num_classes=num_classes, average=\"macro\"),\n",
    "        \"f1_score\": MulticlassF1Score(num_classes=num_classes, average=\"macro\"),\n",
    "        \"confusion_matrix\": MulticlassConfusionMatrix(num_classes=num_classes)\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = net(images)  \n",
    "            preds = torch.argmax(outputs, dim=1)  \n",
    "\n",
    "            for metric in metrics.values():\n",
    "                metric.update(preds, labels)\n",
    "\n",
    "    results = {name: metric.compute() for name, metric in metrics.items()}\n",
    "\n",
    "    for name, value in results.items():\n",
    "        print(f\"Test {name.capitalize()}: {value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.918067216873169\n",
      "Test Recall: 0.9191710948944092\n",
      "Test Precision: 0.8940721154212952\n",
      "Test F1_score: 0.9055185317993164\n",
      "Test Confusion_matrix: tensor([[ 87.,   1.,   1.,   4.],\n",
      "        [  4., 231.,   3.,   9.],\n",
      "        [  0.,   0.,  41.,   1.],\n",
      "        [  4.,   9.,   3.,  78.]])\n"
     ]
    }
   ],
   "source": [
    "test_model(net, test_loader, len(dataset.classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaffliction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
